{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "data_filename = 'C:/Users/User/Desktop/Practical_DS_Leon/Assign_3/data/creditcard.csv'\n",
    "df = pd.read_csv(data_filename)\n",
    "#There are not 'NaNs' in rows\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset normal transactions vs frauds Counter({0: 284315, 1: 492})\n"
     ]
    }
   ],
   "source": [
    "#As we can see we have highly imbalanced dataset\n",
    "from collections import Counter\n",
    "print('Dataset normal transactions vs frauds {}'.format(Counter(df['Class'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some general discussion:\n",
    "\n",
    "Owing to such imbalance in data, an algorithm that does no feature analysis and predicts all the transactions as non-frauds will  achieve an accuracy of 99.828%. Hence, accuracy is not a correct measurement of efficiency in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Time' feature does not indicate the actual time of the transaction but is listing the data in a chronological order. So we assume that 'Time' feature has no significance in classifying a transaction as a fraud. Hence, we eliminate this column from the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Removing 'Time' column from the data as it has no significance in model\n",
    "df = df.drop('Time', axis = 1, errors = 'ignore')\n",
    "\n",
    "#standardize the 'Ammount' feuture because all the other feutures have values arround 0\n",
    "df['Amount']= StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling the dataset:\n",
    "\n",
    "These are techniques that will process the data to have an approximate 50-50 ratio.\n",
    "\n",
    "One way to achieve this is OVER-sampling, this technique adds copies of the under-represented class(better when you have little data).\n",
    "\n",
    "Another is UNDER-sampling, this technique deletes instances from the over-represented class (better when we have a lot of data).\n",
    "In our analysis we will use both balancing techniques so we can compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of normal transactions:  0.5\n",
      "Percentage of fraud transactions:  0.5\n",
      "Total number of transactions in resampled data:  984\n"
     ]
    }
   ],
   "source": [
    "#UNDER-SAMPLE DATAFRAME\n",
    "# Number of data points in the minority class\n",
    "number_records_fraud = len(df[df['Class'] == 1])\n",
    "fraud_indices = df[df['Class'] == 1].index\n",
    "\n",
    "# Picking the indices of the normal classes\n",
    "normal_indices = df[df['Class'] == 0].index\n",
    "\n",
    "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
    "\n",
    "# Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "df_under_sample = df.iloc[under_sample_indices,:]\n",
    "\n",
    "# Showing ratio\n",
    "print(\"Percentage of normal transactions: \", len(df_under_sample[df_under_sample['Class'] == 0])/len(df_under_sample))\n",
    "print(\"Percentage of fraud transactions: \", len(df_under_sample[df_under_sample['Class'] == 1])/len(df_under_sample))\n",
    "print(\"Total number of transactions in resampled data: \", len(df_under_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of normal transactions:  0.5\n",
      "Percentage of fraud transactions:  0.5\n",
      "Total number of transactions in resampled data:  568630\n"
     ]
    }
   ],
   "source": [
    "#OVER-SAMPLE DATAFRAME\n",
    "\n",
    "# Number of data points in the majority class\n",
    "number_records_not_fraud = len(df[df['Class'] == 0])\n",
    "\n",
    "fraud_indices = df[df['Class'] == 1].index\n",
    "normal_indices = df[df['Class'] == 0].index\n",
    "\n",
    "# Picking the indices of the fraud classes\n",
    "fraud_indices = df[df['Class'] == 1].index\n",
    "\n",
    "# Out of the indices we picked, randomly select \"x\" number (number_records_not_fraud)\n",
    "random_fraud_indices = np.random.choice(fraud_indices, number_records_not_fraud, replace = True)\n",
    "\n",
    "# Appending the 2 indices\n",
    "over_sample_indices = np.concatenate([normal_indices, random_fraud_indices])\n",
    "\n",
    "# Over sample dataset\n",
    "df_over_sample = df.iloc[over_sample_indices, :]\n",
    "\n",
    "# Showing ratio\n",
    "print(\"Percentage of normal transactions: \", len(df_over_sample[df_over_sample['Class'] == 0])/len(df_over_sample))\n",
    "print(\"Percentage of fraud transactions: \", len(df_over_sample[df_over_sample['Class'] == 1])/len(df_over_sample))\n",
    "print(\"Total number of transactions in resampled data: \", len(df_over_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL: LOGISTIC REGRESSION\n",
    "\n",
    "Logistic regression calculate the odd ratio of a transaction between the amount of frauds to non-frauds.\n",
    "Consequently, we have to set a threshold that a transcaction will be considered as a fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try logistic regression on Imbalance, Under Sampled and Over Sampled dataframes so we can compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003988\n",
      "         Iterations 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Class</td>      <th>  No. Observations:  </th>  <td>284807</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>284794</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    12</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sat, 13 Jan 2018</td> <th>  Pseudo R-squ.:     </th>  <td>0.6863</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>04:16:00</td>     <th>  Log-Likelihood:    </th> <td> -1135.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -3621.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -8.5209</td> <td>    0.123</td> <td>  -69.282</td> <td> 0.000</td> <td>   -8.762</td> <td>   -8.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>        <td>    0.6240</td> <td>    0.044</td> <td>   14.292</td> <td> 0.000</td> <td>    0.538</td> <td>    0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>        <td>    0.0830</td> <td>    0.026</td> <td>    3.144</td> <td> 0.002</td> <td>    0.031</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>        <td>   -0.1957</td> <td>    0.020</td> <td>   -9.878</td> <td> 0.000</td> <td>   -0.235</td> <td>   -0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>       <td>   -0.4916</td> <td>    0.049</td> <td>  -10.085</td> <td> 0.000</td> <td>   -0.587</td> <td>   -0.396</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>       <td>   -0.3037</td> <td>    0.078</td> <td>   -3.914</td> <td> 0.000</td> <td>   -0.456</td> <td>   -0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>       <td>   -0.6903</td> <td>    0.036</td> <td>  -19.023</td> <td> 0.000</td> <td>   -0.761</td> <td>   -0.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>       <td>   -0.3108</td> <td>    0.053</td> <td>   -5.877</td> <td> 0.000</td> <td>   -0.414</td> <td>   -0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>       <td>   -0.0579</td> <td>    0.033</td> <td>   -1.734</td> <td> 0.083</td> <td>   -0.123</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>       <td>    0.3320</td> <td>    0.052</td> <td>    6.442</td> <td> 0.000</td> <td>    0.231</td> <td>    0.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>       <td>    0.5843</td> <td>    0.116</td> <td>    5.019</td> <td> 0.000</td> <td>    0.356</td> <td>    0.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>       <td>   -0.1395</td> <td>    0.046</td> <td>   -3.024</td> <td> 0.002</td> <td>   -0.230</td> <td>   -0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th>    <td>   -0.0068</td> <td>    0.029</td> <td>   -0.238</td> <td> 0.812</td> <td>   -0.063</td> <td>    0.049</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               284807\n",
       "Model:                          Logit   Df Residuals:                   284794\n",
       "Method:                           MLE   Df Model:                           12\n",
       "Date:                Sat, 13 Jan 2018   Pseudo R-squ.:                  0.6863\n",
       "Time:                        04:16:00   Log-Likelihood:                -1135.9\n",
       "converged:                       True   LL-Null:                       -3621.2\n",
       "                                        LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -8.5209      0.123    -69.282      0.000      -8.762      -8.280\n",
       "V4             0.6240      0.044     14.292      0.000       0.538       0.710\n",
       "V5             0.0830      0.026      3.144      0.002       0.031       0.135\n",
       "V8            -0.1957      0.020     -9.878      0.000      -0.235      -0.157\n",
       "V10           -0.4916      0.049    -10.085      0.000      -0.587      -0.396\n",
       "V13           -0.3037      0.078     -3.914      0.000      -0.456      -0.152\n",
       "V14           -0.6903      0.036    -19.023      0.000      -0.761      -0.619\n",
       "V16           -0.3108      0.053     -5.877      0.000      -0.414      -0.207\n",
       "V20           -0.0579      0.033     -1.734      0.083      -0.123       0.008\n",
       "V21            0.3320      0.052      6.442      0.000       0.231       0.433\n",
       "V22            0.5843      0.116      5.019      0.000       0.356       0.812\n",
       "V23           -0.1395      0.046     -3.024      0.002      -0.230      -0.049\n",
       "Amount        -0.0068      0.029     -0.238      0.812      -0.063       0.049\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.28 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inbalanced model\n",
    "formula = ('Class ~  V4 + V5 + V8 + V10 + \\\n",
    "       V13 + V14 + V16 + V20 + \\\n",
    "       V21 + V22 + V23 + Amount')\n",
    "model = smf.logit(formula, data=df)\n",
    "results = model.fit()\n",
    "odds = results.fittedvalues.apply(lambda x: np.exp(x)).to_frame()\n",
    "df['odds'] = odds\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.137045\n",
      "         Iterations 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Class</td>      <th>  No. Observations:  </th>   <td>568630</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>568601</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>    28</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sat, 13 Jan 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.8023</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>04:12:34</td>     <th>  Log-Likelihood:    </th>  <td> -77928.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-3.9414e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td> 0.000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -3.4683</td> <td>    0.013</td> <td> -271.871</td> <td> 0.000</td> <td>   -3.493</td> <td>   -3.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>        <td>    0.6363</td> <td>    0.014</td> <td>   44.495</td> <td> 0.000</td> <td>    0.608</td> <td>    0.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>        <td>    0.6202</td> <td>    0.020</td> <td>   31.064</td> <td> 0.000</td> <td>    0.581</td> <td>    0.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>        <td>    0.4030</td> <td>    0.011</td> <td>   37.572</td> <td> 0.000</td> <td>    0.382</td> <td>    0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>        <td>    0.7693</td> <td>    0.007</td> <td>  111.723</td> <td> 0.000</td> <td>    0.756</td> <td>    0.783</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>        <td>    0.7101</td> <td>    0.016</td> <td>   43.873</td> <td> 0.000</td> <td>    0.678</td> <td>    0.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>        <td>   -0.5642</td> <td>    0.011</td> <td>  -50.628</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>        <td>   -0.6031</td> <td>    0.019</td> <td>  -31.706</td> <td> 0.000</td> <td>   -0.640</td> <td>   -0.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>        <td>   -0.3979</td> <td>    0.007</td> <td>  -58.417</td> <td> 0.000</td> <td>   -0.411</td> <td>   -0.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>        <td>   -0.2985</td> <td>    0.010</td> <td>  -30.674</td> <td> 0.000</td> <td>   -0.318</td> <td>   -0.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>       <td>   -0.7303</td> <td>    0.013</td> <td>  -56.276</td> <td> 0.000</td> <td>   -0.756</td> <td>   -0.705</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>       <td>    0.5641</td> <td>    0.009</td> <td>   60.342</td> <td> 0.000</td> <td>    0.546</td> <td>    0.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>       <td>   -1.0872</td> <td>    0.014</td> <td>  -77.902</td> <td> 0.000</td> <td>   -1.115</td> <td>   -1.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>       <td>   -0.3612</td> <td>    0.007</td> <td>  -53.610</td> <td> 0.000</td> <td>   -0.374</td> <td>   -0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>       <td>   -1.3587</td> <td>    0.016</td> <td>  -85.681</td> <td> 0.000</td> <td>   -1.390</td> <td>   -1.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>       <td>   -0.0822</td> <td>    0.007</td> <td>  -11.261</td> <td> 0.000</td> <td>   -0.097</td> <td>   -0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>       <td>   -0.7188</td> <td>    0.015</td> <td>  -49.375</td> <td> 0.000</td> <td>   -0.747</td> <td>   -0.690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>       <td>   -0.8108</td> <td>    0.020</td> <td>  -40.690</td> <td> 0.000</td> <td>   -0.850</td> <td>   -0.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>       <td>   -0.3265</td> <td>    0.012</td> <td>  -27.074</td> <td> 0.000</td> <td>   -0.350</td> <td>   -0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>       <td>    0.3063</td> <td>    0.010</td> <td>   30.555</td> <td> 0.000</td> <td>    0.287</td> <td>    0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>       <td>   -0.8619</td> <td>    0.019</td> <td>  -44.562</td> <td> 0.000</td> <td>   -0.900</td> <td>   -0.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>       <td>    0.0166</td> <td>    0.009</td> <td>    1.851</td> <td> 0.064</td> <td>   -0.001</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>       <td>    0.7135</td> <td>    0.012</td> <td>   60.086</td> <td> 0.000</td> <td>    0.690</td> <td>    0.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>       <td>    0.4630</td> <td>    0.019</td> <td>   24.904</td> <td> 0.000</td> <td>    0.427</td> <td>    0.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>       <td>   -0.0917</td> <td>    0.014</td> <td>   -6.720</td> <td> 0.000</td> <td>   -0.118</td> <td>   -0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>       <td>    0.1489</td> <td>    0.014</td> <td>   10.457</td> <td> 0.000</td> <td>    0.121</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>       <td>   -0.3679</td> <td>    0.016</td> <td>  -22.559</td> <td> 0.000</td> <td>   -0.400</td> <td>   -0.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>       <td>    0.8062</td> <td>    0.031</td> <td>   26.280</td> <td> 0.000</td> <td>    0.746</td> <td>    0.866</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th>    <td>    2.1671</td> <td>    0.051</td> <td>   42.383</td> <td> 0.000</td> <td>    2.067</td> <td>    2.267</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               568630\n",
       "Model:                          Logit   Df Residuals:                   568601\n",
       "Method:                           MLE   Df Model:                           28\n",
       "Date:                Sat, 13 Jan 2018   Pseudo R-squ.:                  0.8023\n",
       "Time:                        04:12:34   Log-Likelihood:                -77928.\n",
       "converged:                       True   LL-Null:                   -3.9414e+05\n",
       "                                        LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -3.4683      0.013   -271.871      0.000      -3.493      -3.443\n",
       "V1             0.6363      0.014     44.495      0.000       0.608       0.664\n",
       "V2             0.6202      0.020     31.064      0.000       0.581       0.659\n",
       "V3             0.4030      0.011     37.572      0.000       0.382       0.424\n",
       "V4             0.7693      0.007    111.723      0.000       0.756       0.783\n",
       "V5             0.7101      0.016     43.873      0.000       0.678       0.742\n",
       "V6            -0.5642      0.011    -50.628      0.000      -0.586      -0.542\n",
       "V7            -0.6031      0.019    -31.706      0.000      -0.640      -0.566\n",
       "V8            -0.3979      0.007    -58.417      0.000      -0.411      -0.385\n",
       "V9            -0.2985      0.010    -30.674      0.000      -0.318      -0.279\n",
       "V10           -0.7303      0.013    -56.276      0.000      -0.756      -0.705\n",
       "V11            0.5641      0.009     60.342      0.000       0.546       0.582\n",
       "V12           -1.0872      0.014    -77.902      0.000      -1.115      -1.060\n",
       "V13           -0.3612      0.007    -53.610      0.000      -0.374      -0.348\n",
       "V14           -1.3587      0.016    -85.681      0.000      -1.390      -1.328\n",
       "V15           -0.0822      0.007    -11.261      0.000      -0.097      -0.068\n",
       "V16           -0.7188      0.015    -49.375      0.000      -0.747      -0.690\n",
       "V17           -0.8108      0.020    -40.690      0.000      -0.850      -0.772\n",
       "V18           -0.3265      0.012    -27.074      0.000      -0.350      -0.303\n",
       "V19            0.3063      0.010     30.555      0.000       0.287       0.326\n",
       "V20           -0.8619      0.019    -44.562      0.000      -0.900      -0.824\n",
       "V21            0.0166      0.009      1.851      0.064      -0.001       0.034\n",
       "V22            0.7135      0.012     60.086      0.000       0.690       0.737\n",
       "V23            0.4630      0.019     24.904      0.000       0.427       0.499\n",
       "V24           -0.0917      0.014     -6.720      0.000      -0.118      -0.065\n",
       "V25            0.1489      0.014     10.457      0.000       0.121       0.177\n",
       "V26           -0.3679      0.016    -22.559      0.000      -0.400      -0.336\n",
       "V28            0.8062      0.031     26.280      0.000       0.746       0.866\n",
       "Amount         2.1671      0.051     42.383      0.000       2.067       2.267\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.41 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Over-Sample logistic-regression\n",
    "formula = ('Class ~  V1 + V2+ V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + \\\n",
    "       V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + \\\n",
    "       V21 + V22 + V23 + V24 + V25 + V26 + V28 + Amount')\n",
    "\n",
    "#We used all the feutures and we observed that V27 wasn't statistically signicant correlated with the dependend variable\n",
    "#We removed V27 and we run the model again\n",
    "#Now all the variables are statistically significant for our model\n",
    "#It is possible sometimes when we remove one variables the p-values of other feutures to change so we have to rexamine our model!\n",
    "model = smf.logit(formula, data=df_over_sample)\n",
    "results = model.fit()\n",
    "#'Un-logarithm' the results so we can intepret them better\n",
    "odds = results.fittedvalues.apply(lambda x: np.exp(x)).to_frame()\n",
    "\n",
    "df_over_sample['odds'] = odds\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.159195\n",
      "         Iterations 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Class</td>      <th>  No. Observations:  </th>   <td>   984</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   973</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>    10</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sat, 13 Jan 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.7703</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>04:17:44</td>     <th>  Log-Likelihood:    </th>  <td> -156.65</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -682.06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>2.103e-219</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -3.0567</td> <td>    0.229</td> <td>  -13.350</td> <td> 0.000</td> <td>   -3.505</td> <td>   -2.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>        <td>    0.9062</td> <td>    0.108</td> <td>    8.373</td> <td> 0.000</td> <td>    0.694</td> <td>    1.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>        <td>    0.0668</td> <td>    0.088</td> <td>    0.759</td> <td> 0.448</td> <td>   -0.106</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>        <td>   -0.2787</td> <td>    0.081</td> <td>   -3.436</td> <td> 0.001</td> <td>   -0.438</td> <td>   -0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>       <td>   -0.5243</td> <td>    0.167</td> <td>   -3.144</td> <td> 0.002</td> <td>   -0.851</td> <td>   -0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>       <td>   -0.3577</td> <td>    0.161</td> <td>   -2.222</td> <td> 0.026</td> <td>   -0.673</td> <td>   -0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>       <td>   -0.8245</td> <td>    0.137</td> <td>   -6.031</td> <td> 0.000</td> <td>   -1.092</td> <td>   -0.557</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>       <td>   -0.4134</td> <td>    0.216</td> <td>   -1.916</td> <td> 0.055</td> <td>   -0.836</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>       <td>    0.2578</td> <td>    0.203</td> <td>    1.272</td> <td> 0.203</td> <td>   -0.140</td> <td>    0.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>       <td>   -0.3179</td> <td>    0.128</td> <td>   -2.490</td> <td> 0.013</td> <td>   -0.568</td> <td>   -0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th>    <td>    0.5308</td> <td>    0.154</td> <td>    3.440</td> <td> 0.001</td> <td>    0.228</td> <td>    0.833</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:                  984\n",
       "Model:                          Logit   Df Residuals:                      973\n",
       "Method:                           MLE   Df Model:                           10\n",
       "Date:                Sat, 13 Jan 2018   Pseudo R-squ.:                  0.7703\n",
       "Time:                        04:17:44   Log-Likelihood:                -156.65\n",
       "converged:                       True   LL-Null:                       -682.06\n",
       "                                        LLR p-value:                2.103e-219\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -3.0567      0.229    -13.350      0.000      -3.505      -2.608\n",
       "V4             0.9062      0.108      8.373      0.000       0.694       1.118\n",
       "V5             0.0668      0.088      0.759      0.448      -0.106       0.239\n",
       "V8            -0.2787      0.081     -3.436      0.001      -0.438      -0.120\n",
       "V10           -0.5243      0.167     -3.144      0.002      -0.851      -0.197\n",
       "V13           -0.3577      0.161     -2.222      0.026      -0.673      -0.042\n",
       "V14           -0.8245      0.137     -6.031      0.000      -1.092      -0.557\n",
       "V20           -0.4134      0.216     -1.916      0.055      -0.836       0.010\n",
       "V22            0.2578      0.203      1.272      0.203      -0.140       0.655\n",
       "V23           -0.3179      0.128     -2.490      0.013      -0.568      -0.068\n",
       "Amount         0.5308      0.154      3.440      0.001       0.228       0.833\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.22 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Under-Sample logistic-regression\n",
    "#after some trial and error we kept only the feutures that their correlations are statistically significant in our model.\n",
    "formula = ('Class ~  V4 + V5 + V8 + V10 + \\\n",
    "       V13 + V14 + V20 + \\\n",
    "       V22 + V23 + Amount')\n",
    "\n",
    "model = smf.logit(formula, data=df_under_sample)\n",
    "results = model.fit()\n",
    "#'Un-logarithm' the results so we can intepret them better\n",
    "odds = results.fittedvalues.apply(lambda x: np.exp(x)).to_frame()\n",
    "df_under_sample['odds'] = odds\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELS EVALUATION\n",
    "\n",
    "Acuracy Score:\n",
    "\n",
    "As mentioned above, accuracy score is a useless evaluation for our imbalance dataframe but it may be useful on the resampled dfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and Recall:\n",
    "\n",
    "Precision denotes the probability that a transaction that is classified as fraud is truly a fraud.\n",
    "Recall (aka. True Positive Rate) is the probability that a true fraud is recognized by the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: Out of the frauds we found how many are real frauds. Frauds identified / Actual frauds indentified \n",
    "\n",
    "Recall : How many of the real frauds we found. Actual frauds identidied / Total Frauds\n",
    "\n",
    "It is obvious that there is a tradeoff between those two definition.\n",
    "If we want recall = 1 then probably we have to increase the number of transactions predicted as frauds.\n",
    "\n",
    "In our case, because we are predicting credit frauds, recall maybe is more important because we want to avoid frauds.\n",
    "On the other hand, we do not want to block too many transactions that we are not sure about their fraudment and make user's life difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function to calculate precision and recalls for different thresholds\n",
    "def model_evaluation(df , odd_threshold):\n",
    "    #we determine the transactions' prediction as fraud or non-fraud depending on the threshold that have been given.\n",
    "    df['class_predict'] = df['odds'].apply(lambda odd: 1 if odd > odd_threshold else 0)\n",
    "    #count the number of transactions that have been predicted as frauds\n",
    "    frauds_identified = df['class_predict'].sum()\n",
    "    #count the number of frauds that have been predicted correctly (True-Positive) \n",
    "    actual_frauds_ident = len(df.loc[(df['Class'] == 1) & (df['class_predict'] == 1), :])\n",
    "    #count the number of all the actual frauds\n",
    "    total_actual_frauds = df['Class'].sum()\n",
    "    #count the false_positives\n",
    "    false_positive = len(df.loc[(df['Class'] == 0) & (df['class_predict'] == 1), :])\n",
    "    #count the false_negatives\n",
    "    false_negative = len(df.loc[(df['Class'] == 1) & (df['class_predict'] == 0), :])\n",
    "    \n",
    "    \n",
    "    Accuracy = len(df.loc[df['Class'] == df['class_predict'], :])/ len(df)\n",
    "    Precision = actual_frauds_ident / frauds_identified\n",
    "    Recall = actual_frauds_ident / total_actual_frauds\n",
    "    \n",
    "    print('Precision: ',format(Precision, '.3f'))\n",
    "    print('Recall:    ', format(Recall, '.3f'))\n",
    "    print('Accuracy:  ', format(Accuracy, '.3f'))\n",
    "    print('False-Positive: ', false_positive)\n",
    "    print('False-Negative: ', false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalanced Dataframe\n",
      "Precision:  0.891\n",
      "Recall:     0.600\n",
      "Accuracy:   0.999\n",
      "False-Positive:  36\n",
      "False-Negative:  197\n",
      "\n",
      "Over Sampled Dataframe\n",
      "Precision:  0.983\n",
      "Recall:     0.913\n",
      "Accuracy:   0.949\n",
      "False-Positive:  4470\n",
      "False-Negative:  24795\n",
      "\n",
      "Under Sample Dataframe\n",
      "Precision:  0.984\n",
      "Recall:     0.902\n",
      "Accuracy:   0.944\n",
      "False-Positive:  7\n",
      "False-Negative:  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#print the results so we can compare them\n",
    "#give different thresholds to compare the tradeoffs between precision/recall-false_positive/false_negative\n",
    "print('Imbalanced Dataframe')\n",
    "model_evaluation(df , 1.5)\n",
    "print()\n",
    "print('Over Sampled Dataframe')\n",
    "model_evaluation(df_over_sample , 1.5)\n",
    "print()\n",
    "print('Under Sample Dataframe')\n",
    "model_evaluation(df_under_sample, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we explained above, on the imbalanced df the accuracy is extremly hight but the recall and precision are bad!!!\n",
    "\n",
    "On the other two dataframes the results look really good. We can tune the threshold to find the balance point between precision and recall that we prefer.\n",
    "\n",
    "We should remember that in this exercise we use the same df for trainig and testing. This is not a good practise because of overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
